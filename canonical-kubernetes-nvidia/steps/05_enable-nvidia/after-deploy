#!/bin/bash

set -eux

. $CONJURE_UP_SPELLSDIR/sdk/common.sh

# Append our needed config to any existing kubelet-extra-args
kublet_current_config="$(juju config -m $JUJU_CONTROLLER:$JUJU_MODEL kubernetes-worker kubelet-extra-args)"
kubelet_needed_config="feature-gates=DevicePlugins=true"
if $(echo "${kublet_current_config}" | grep -q -- "${kubelet_needed_config}"); then
    echo "Required kubelet-extra-args config is already set"
else
    echo "Configuring kubelet-extra-args for kubernetes-worker"
    juju config -m $JUJU_CONTROLLER:$JUJU_MODEL kubernetes-worker \
        kubelet-extra-args="${kublet_current_config} ${kubelet_needed_config}"
fi

# Don't bother checking existing docker_runtime; we always want it to be 'auto'
echo "Configuring docker_runtime for kubernetes-worker"
juju config -m $JUJU_CONTROLLER:$JUJU_MODEL kubernetes-worker \
    docker_runtime="auto"

# Ensure our libcuda symlinks are correct. The existing cuda layer hard codes
# a versioned symlink, but the latest cuda is a newer version:
# https://github.com/juju-solutions/layer-nvidia-cuda/blob/master/reactive/cuda.sh#L241
echo "Updating libnvcuvid symlinks"
juju run -m $JUJU_CONTROLLER:$JUJU_MODEL --application kubernetes-worker \
    'ln -sf /usr/lib/nvidia-*/libnvcuvid.so.1 /usr/lib/libnvcuvid.so.1'
juju run -m $JUJU_CONTROLLER:$JUJU_MODEL --application kubernetes-worker \
    'ln -sf /usr/lib/nvidia-*/libnvcuvid.so /usr/lib/libnvcuvid.so'

# Step 02 should have put kubectl in $HOME/bin; use it or explain what to do.
echo "Deploying nVidia daemonset"
if [[ -x $HOME/bin/kubectl ]]; then
    $HOME/bin/kubectl create -f $(scriptPath)/nvidia-device-plugin.yml
    setResult "Kubernetes workers have been configured and the nVidia daemonset deployed."
else
    setResult "Could not find kubectl. You must deploy the nVidia daemonset: \
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml"
fi

exit 0
